{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNfZPg4sHCFJ7lQumeiUouw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Mount the Google Drive"],"metadata":{"id":"VwEsVeKwK6tQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MkjPtqfK6OQ"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","source":["!python -m pip install --upgrade pip"],"metadata":{"id":"20M9UuxqS11l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python -m pip install --user spacy==3.1.3"],"metadata":{"id":"nd2avvbhLG2h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. Import the required libraries"],"metadata":{"id":"smGvs4ZBLM0e"}},{"cell_type":"code","source":["import re\n","import spacy"],"metadata":{"id":"4EIjI65PLLJN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -U wn==0.0.22"],"metadata":{"id":"2lCx764nMaeI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip show nltk"],"metadata":{"id":"ys3FB28qM1H4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python --version"],"metadata":{"id":"OcGBhzRFNA74"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download(\"popular\")"],"metadata":{"id":"rlOMBWyEVaGj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","import string\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","import numpy as np\n","import pandas as pandas\n","\n","from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","from warnings import filterwarnings\n","filterwarnings('ignore')"],"metadata":{"id":"edPyniZuLZaK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python -m spacy download en_core_web_sm"],"metadata":{"id":"shNATplyNSjI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nlp=spacy.load('en_core_web_sm')"],"metadata":{"id":"_k8hWP_LLnc6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/Topic_Modelling/Wells_Fargo_transcript.txt\",\"r\") as f:\n","  D1 = f.read()\n","print(D1)"],"metadata":{"id":"6u4D7-LSPYfs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/Topic_Modelling/Cisco_transcript.txt\",\"r\") as f:\n","  D2 = f.read()\n","print(D2)"],"metadata":{"id":"njtAfDBEPYGF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/Topic_Modelling/Intuit_transcript.txt\",\"r\") as f:\n","  D3 = f.read()\n","print(D3)"],"metadata":{"id":"as_hK8HEPX-9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/Topic_Modelling/UHG_transcript.txt\",\"r\") as f:\n","  D4 = f.read()\n","print(D4)"],"metadata":{"id":"I409Z5VXPX3_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/drive/MyDrive/Topic_Modelling/Thermo_Fisher_transcript.txt\",\"r\") as f:\n","  D5 = f.read()\n","print(D5)"],"metadata":{"id":"8HlL_4l-NQWF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print ('D1: ',D1,'\\nD2: ',D2,'\\nD3: ',D3,'\\nD4: ',D4,'\\nD5: ',D5, end = \"\\n\",)"],"metadata":{"id":"-8KG0eZ2PhC4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combine all the documents into a list:\n","corpus = [D1, D2, D3, D4, D5]\n","print( \"Corpus: \", corpus)"],"metadata":{"id":"5K1nwYIzPf4e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Text Preprocessing"],"metadata":{"id":"Q0o9XUy8NnlM"}},{"cell_type":"code","source":["stop_loss_words = set(stopwords.words('english'))\n","exclude = set(string.punctuation)\n","lemma = WordNetLemmatizer()\n","\n","\n","def clean_data(doc):\n","  # Convert text into lower case and split into words\n","  stop_free_word = \" \".join([i for i in doc.lower().split() if i not in stop_loss_words])\n","\n","  # Remove stop words if present\n","  remove_stop_words = ''.join(ch for ch in stop_free_word if ch not in exclude)  \n","\n","  # Remove punctuations, symbols and special characters and normalize the text\n","  normalize_text = \" \".join(lemma.lemmatize(word) for word in remove_stop_words.split())  \n","  return normalize_text\n","\n","# Clean data is stored in a new list\n","clean_corpus = [clean_data(doc).split() for doc in corpus]\n","print(\"Clean corpus: \", clean_corpus)"],"metadata":{"id":"WqmXokh9NlvB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Convert Text into Numerical Representation"],"metadata":{"id":"mO0e0K0eOptx"}},{"cell_type":"code","source":["# Converting text into numerical representation using tf-idf vectorizer\n","tf_idf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False) \n","print('TF-IDF Vectorizer: ',tf_idf_vectorizer)\n","# Converting text into numerical representation using count vectorizer\n","cv_vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n","print('Count Vectorizer: ',cv_vectorizer)"],"metadata":{"id":"P8PDQCgiOhge"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Converting text into numerical representation using tf-idf vectorizer\n","tf_idf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False) \n","print('TF-IDF Vectorizer: ',tf_idf_vectorizer)"],"metadata":{"id":"tDdl_4q5uSnw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Array from TF-IDF Vectorizer (Convert to Document-Term Matrix)\n","tf_idf_array = tf_idf_vectorizer.fit_transform(clean_corpus)\n","print(tf_idf_array)"],"metadata":{"id":"0SxjCZ9JO24O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf_idf_array"],"metadata":{"id":"zlzxSTLfWhuq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Array from Count Vectorizer (Convert to Document-Term Matrix)\n","cv_array = cv_vectorizer.fit_transform(clean_corpus)\n","print(cv_array)"],"metadata":{"id":"EfztBw6uO-rt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cv_array"],"metadata":{"id":"kxl0oW9PWjSC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating vocabulary array from tf-idf\n","vocab_tf_idf = tf_idf_vectorizer.get_feature_names()\n","print(vocab_tf_idf)"],"metadata":{"id":"cT2pj1zqPc03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_tf_idf"],"metadata":{"id":"iuammvrGWlNa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating vocabulary array from cv\n","vocab_cv = cv_vectorizer.get_feature_names()\n","print(vocab_cv)"],"metadata":{"id":"RwWb_F4zPlUu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_cv"],"metadata":{"id":"gnbrv214WoGy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(\"Length of vocabulary array using tf_idf: \", len(vocab_tf_idf))\n","display(\"Length of vocabulary array using cv: \",len(vocab_cv))"],"metadata":{"id":"b3Zw80VFPlRX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. LDA Algorithm"],"metadata":{"id":"tl1iIhCMPy1u"}},{"cell_type":"code","source":["# Create object for the LDA class \n","lda_algorithm_cv = LatentDirichletAllocation(n_components = 20, max_iter = 20, random_state = 20)\n","print(\"LDA Algorithm : \",lda_algorithm_cv)\n","# fit transform on model on our cv_vectorizer\n","X_topics = lda_algorithm_cv.fit_transform(cv_array)\n","print(\"X Topics : \",X_topics)\n","\n","# .components_ gives us our topic distribution \n","topic_words = lda_algorithm_cv.components_\n","print( 'Topic Words : ', topic_words)"],"metadata":{"id":"95n1p6Hmqi3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create object for the LDA class \n","lda_algorithm_tfidf = LatentDirichletAllocation(n_components = 20, max_iter = 20, random_state = 20)\n","print(\"LDA Algorithm : \",lda_algorithm_tfidf)\n","# fit transform on model on our tf_idf_vectorizer\n","X_topics1 = lda_algorithm_tfidf.fit_transform(tf_idf_array)\n","print(\"X Topics : \",X_topics1)\n","\n","# .components_ gives us our topic distribution \n","topic_words1 = lda_algorithm_tfidf.components_\n","print( 'Topic Words : ', topic_words1)"],"metadata":{"id":"R74KWBCEPlMe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.1) Retrieve the Topics"],"metadata":{"id":"HJHEi1X4QDzW"}},{"cell_type":"code","source":["# Initialize the number of words \n","n_top_words = 10\n","for i, topic_list in enumerate (topic_words):\n","\n","  # Sorting an array or a list or the matrix according to their values\n","  sorted_topic_list = np.argsort(topic_list)\n","\n","  # View the actual words present in those indexes\n","  topic_words = np.array(vocab_cv)[sorted_topic_list]\n","\n","  # topic_words variable contains the Topics and respective words present in those Topics\n","  topic_words = topic_words[:-n_top_words:-1]\n","\n","  print (\"Topic\", str(i+1), topic_words)"],"metadata":{"id":"Gs3Jdc7_0re5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize the number of words \n","n_top_words = 10\n","for i, topic_list in enumerate (topic_words1):\n","\n","  # Sorting an array or a list or the matrix according to their values\n","  sorted_topic_list = np.argsort(topic_list)\n","\n","  # View the actual words present in those indexes\n","  topic_words1 = np.array(vocab_tf_idf)[sorted_topic_list]\n","\n","  # topic_words variable contains the Topics and respective words present in those Topics\n","  topic_words1 = topic_words1[:-n_top_words:-1]\n","\n","  print (\"Topic\", str(i+1), topic_words1)"],"metadata":{"id":"WEcxxjtAQDU6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.2) Annotate the Topic documents"],"metadata":{"id":"C4v_5vZxQcOm"}},{"cell_type":"code","source":["document_topic = lda_algorithm_cv.transform(cv_array)\n","\n","for l in range(document_topic.shape[0]):\n","  topic_document = document_topic[l].argmax()\n","\n","  print(\" Document \", l+1, \" --> Topic : \",topic_document )"],"metadata":{"id":"Xt5ntcxgq6Je"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["document_topic = lda_algorithm_tfidf.transform(tf_idf_array)\n","\n","for l in range(document_topic.shape[0]):\n","  topic_document = document_topic[l].argmax()\n","\n","  print(\" Document \", l+1, \" --> Topic : \",topic_document )"],"metadata":{"id":"hUlRvlzBQf_L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5) Using PyLDAvis for Visualization"],"metadata":{"id":"mPex1LrLptzD"}},{"cell_type":"code","source":["!pip install pyLDAvis"],"metadata":{"id":"8DRkgoBfpxV5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from __future__ import print_function"],"metadata":{"id":"vLyeZQ_YpxTH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pyLDAvis\n","import pyLDAvis.sklearn\n","pyLDAvis.enable_notebook()"],"metadata":{"id":"H-uzYLhipxCH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pyLDAvis.sklearn.prepare(lda_algorithm_cv, cv_array, cv_vectorizer)"],"metadata":{"id":"F8KdJ8lWp2I4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pyLDAvis.sklearn.prepare(lda_algorithm_tfidf, tf_idf_array, tf_idf_vectorizer)"],"metadata":{"id":"FlfSaqewp2Fa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pyLDAvis.sklearn.prepare(lda_algorithm_cv, cv_array, cv_vectorizer,mds='mmds')"],"metadata":{"id":"1l4xkKXGviKg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pyLDAvis.sklearn.prepare(lda_algorithm_cv, cv_array, cv_vectorizer,mds='tsne')"],"metadata":{"id":"9uwdAA17vnzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pyLDAvis.sklearn.prepare(lda_algorithm_tfidf, tf_idf_array, tf_idf_vectorizer,mds='mmds')"],"metadata":{"id":"cWh0JLsIvjE3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pyLDAvis.sklearn.prepare(lda_algorithm_tfidf, tf_idf_array, tf_idf_vectorizer,mds='tsne')"],"metadata":{"id":"mvqeSrqqvoIo"},"execution_count":null,"outputs":[]}]}